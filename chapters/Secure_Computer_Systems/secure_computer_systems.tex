% cSpell:ignore TCSEC

\graphicspath{ {./chapters/Secure_Computer_Systems} }

\section{Introduction to Secure Computer Systems}
\label{sec:intro_to_secure_comp_systems}
  There are threats to a cyber system. 
  Specifically these can be summarized using the security mindset:
  \begin{itemize}
    \item \textbf{Threats} - Standard suite of bad-actors
    \item \textbf{Vulnerabilities} - Complexity of OS
    \item \textbf{Attacks} - Attacks against an OS are particularly attractive as they provide access to everything protected by the OS.
  \end{itemize}

  \subsection{Trusted Computing Base: An Overview}
  \label{ssec:tcb_overview}

    An operating system is a set of software which makes it easier to use and share physical resources.
    These resources can range from Memory to CPU Cores to physical IO.
    The OS also manages these resources and can restrict access to them.
    In order to accomplish this an OS must have access to all the physical resources.
    Because of its complete access to all physical units the operating system provides access to all
      the physical components of the system and through this can also access all of the data processed by
      the system.
    In a secure system the OS can be considered/made to be a trusted computing base.

    \begin{defbox}[Trusted Computing Base]
      A software suite (usually and operating system) upon which other programs
        run that must be trusted for the security of said programs.
      (i.e. a software \textit{BASE} which can be used and \textit{TRUSTED} by other programs)
    \end{defbox}

    Trusted Computing Bases (TCBs) have several key functions, but the foremost is arguably that of
      a reference monitor.
    In it's role as a reference monitor a TCB maintains limited access to certain resources by the various programs
      which run on the TCB.
    That is to say that the TCB must monitor and control program references to these resources.
    It is critical that no reference can bypass a TCB, else the TCB loses its reference monitor capability and 
      is defeated.

    A properly implemented TCB has a handful of requirements:
    \begin{enumerate}
      \item \textbf{Tamper-Proof}
      \begin{itemize}
        \item Untrusted code cannot be able to operate the TCB
      \end{itemize}
      \item \textbf{Complete Mediation}
      \begin{itemize}
        \item Untrusted applications must be unable to access protected resources without going through the TCB
        \item The TCB cannot be bypassed by an untrusted program
      \end{itemize}
      \item \textbf{Correctness}
      \begin{itemize}
        \item A TCB shall have no known vulnerabilities
        \item A TCB vulnerability negates the previous two requirements.
      \end{itemize}
    \end{enumerate}

    Now it should be noted that a TCB does rely on hardware.
    Trust in hardware is its own challenge and beyond the scope of this chapter (see \autoref{chap:hardware_trust} for more details).
    Thus for the remainder of this chapter it is prudent to assume the hardware is trusted.

    \cite{cs6238_Ahamad}

  \subsection{TCB as a Reference Monitor}
  \label{ssec:tcb_as_ref_monitor}

    When acting as a reference monitor what tasks should the TCB perform?

    \begin{enumerate}
      \item \textbf{Authentication}
      \begin{itemize}
        \item Who is the source of the request?
      \end{itemize}
      \item \textbf{Authorization}
      \begin{itemize}
        \item Does the source of the request have permission to access the resource?
      \end{itemize}
      \item \textbf{Audit}
      \begin{itemize}
        \item Can a admin check the above two requirements have been adhered to so far?
      \end{itemize}
    \end{enumerate}

    In order for a TCB to act as a reference monitor the TCB must be able to perform the above tasks in order to ensure trust.

    \cite{cs6238_Ahamad}

  \subsection{Requirements for Trustworthiness}
  \label{ssec:reqs_for_trustworthiness}

    How do we decide to trust a TCB?
    Are there certain attributes which contribute to our trust of a TCB?

    Generally trust in a TCB comes from two places.
    The first is what the TCB does.
    These are the claims about what the TCB is able to do and its reported limitations.
    The second is how well it does these things.
    It does not particularly matter how secure or trustworthy a TCB claims to be if
      it doesn't actually follow through on those promises.
    What the TCB does is pretty easy to judge, but how well is more difficult.

    There are some things which can be analyzed to help support the second consideration.
    One of these is how the TCB is structured.
    Structuring allows for defense in depth and other strategies to be analyzed.
    Testing and formal verification are other strong metrics which allow 

    \cite{cs6238_Ahamad}

    These cause other questions to arise.
    Can we truly trust any code which we have not written? 
    It would seem no unless you are starting from the very base and writing everything yourself
      (of course even here it is impossible to fully trust the code as a single human is bound.
      to have made a mistake or two)
    Now one is tempted to reject this on the notion that if the source code can be reviewed for
      trojans or other "bugs" then the code can indeed be trusted.
    However, in practice this is not true as Ken Thompson pointed out in his life time achievement award.
    It is entirely possible to be using a tool which is built so as to re-insert a trojan when it rebuilds
      itself or introduce a different trojan when building any other programs. 
    This problem forces complete trust to be derived only from a machine-code base up system which is entirely
      developed by yourself for complete trust.
    Alternatively, anyone who has touched the TCB or any part of the tool-chain to arrive at the TCB must be
      trusted implicitly.
    \cite{thompson1983reflections}

  \subsection{TCSEC: The Orange Book}
  \label{ssec:tcsec_the_orange_book}

    So if perfect trust is so hard to attain are there questions one can ask to determine whether
      a TCB can truly be trusted?
    
    In the mid-1980s the DoD Security center published the \textit{Trusted Computer System Evaluation Criteria}, 
      colloquially known as "The Orange Book".
    This document puts forward a list of questions and requirements for certain levels of trust
      used by US Government secure computer systems. \cite{book1985_tcsec}

    The Orange Book puts forward set of classes which evaluate the trust of a system as discussed below.

    \begin{itemize}
      \item \textbf{Class D}
      \begin{itemize}
        \item Fails to meet the minimum requirements of TCB
      \end{itemize}
      \item \textbf{Class C1}
      \begin{itemize}
        \item Isolation of TCB
        \item User Authentication
        \item Access Control (discretionary)
      \end{itemize}
      \item \textbf{Class C2}
      \begin{itemize}
        \item All the features of C1
        \item Accountability and Audit Requirements
        \item Logging capability
      \end{itemize}
      \item \textbf{Class B1}
      \begin{itemize}
        \item All the features of C2
        \item Well-defined TCB
        \item Access Control (mandatory)
        \item Penetration Testing
      \end{itemize}
      \item \textbf{Class B2}
      \begin{itemize}
        \item All the features of B1
        \item Confinement and Covert Channels
        \item Well Structured TCB (such as modularity)
      \end{itemize}
      \item \textbf{Class B3}
      \begin{itemize}
        \item All the features of B2
        \item Well defined security model
        \item Separation of security code
        \item Least Privilege
      \end{itemize}
      \item \textbf{Class A1}
      \begin{itemize}
        \item All the features of B3
        \item Formal design verification
      \end{itemize}
      \item \textbf{Class A2}
      \begin{itemize}
        \item All the features of A1
        \item Formal implementation verification
      \end{itemize}
    \end{itemize}

  \subsection{Introduction to Trusted Platform Module}
  \label{ssec:intro_to_tpm}

    Having software which exists at the TCSEC \textit{A2} level is all well and good, but at some-point
      that TCB has to be loaded and booted.
    How do we know some bad actor has not changed the TCB and we are not booting a malicious or modified TCB?
    One option is the Trusted Platform Module (TPM).

    \begin{defbox}[Trusted Platform Module]
      A hardware "root" of trust which is included in a computer system.

      It performs an attestation of the operating system and platform by checking a digest 
        of the TCB which is cryptographically strong.
    \end{defbox}

    In theory this solves the immediate issue since we are considering hardware trusted;
      however, TPMs are not without their own draw backs.
    TPMs allow hardware and OEM vendors to control what sorts of software (and OS) can be used
      on a specific hardware platform since other software would not pass the attestation of the TPM.
    This can lead to companies such as Apple locking users out of using their device except with authentic iOS
      or Apple signed software.

\section{Design Principles for Secure Systems}
\label{sec:design_principles_for_sec_systems}

  Secure systems are complex and must accomplish a wide variety of tasks to be successful.
  If our task were to create an extremely simple system secure and trustworthy it would be relatively 
    straight forward; however operating systems, and by extension TCB, are extremely complex.
  For a perfectly trustworthy system there should be no vulnerabilities or errors, since a single
    vulnerability is all that an adversary may need to break trust.
  So, how do we minimize the likelihood of security security weaknesses?
  The short answer is by following a key set of design principles.

  \subsection{Real-World Security}
  \label{ssec:real_world_security}

    Physical security has been around far longer than secure computer systems, so let's start by
      looking at it for some basic principles.
    In the real-world only valuables are secured, and the more valuable the more secure we are likely
      to make the item.
    These valuables tend to be stolen for profit, though some items also may be targets for vandalism.
    The type of threat against the item determines the type of security which is necessary.
    Additionally, as we add more security to an item the types of threat actor which we encounter changes.
    For example banks are very secure, but also provide the opportunity for high-profit ventures,
      thus we can expect a smaller but more skilled group of thieves to continue to target the bank.
    Knowing this make up of threat actors and already implemented defenses allows a physical security
      firm to make better decisions about what new security features to add.

  \subsection{Design Principle 1: Security Cost}
  \label{ssec:design_principle_1_security_cost}

    The first design principle which should be adhered to is the economics of security.
    Security cost must be commensurate with the treat level and asset value.
    Fundamentally, it makes sense that we should not spend more money trying to secure an item than
      that item is worth.
    Likewise, it is not necessary to make an item impossible to steal, instead it just needs to be 
      expensive enough and risky enough that it is not worth it for a thief to try and steal it.
    But to go beyond that we also recognize that the more likely an item is to be targeted the more we need
      to spend to ensure its protection.
    From a defense standpoint this can be expressed mathematically.
    We would like to add defenses until:

    \begin{equation}
      C_D < V_D \text{ and } C_A > V_A
    \end{equation}
    
    where $C_D$ is the cost to the defender, $V_D$ is the items value to the defender, $C_A$ is the
      cost to the attacker, and $V_A$ is the value to the attacker.

    \begin{keybox}[Design Principle 1: Security Cost]
      Security cost must be commensurate with the treat level and asset value.
    \end{keybox}

  \subsection{Design Principle 2: User Acceptability}
  \label{ssec:design_principle_2_user_Acceptability}

    In addition to the economic cost of security there is another lest quantifiable cost: usability.
    Ideally security does not interfere with the usability of the system, but in practice this cannot be 
      the case.
    For example password requirements and multi-factor authentication can greatly improve security,
      but they also add burden to a standard user.

    In a perfect world all of these additional security measures would be handled automatically.
    However, systems are only secure as the weakest link.
    More and more often these days that weak link is the user.
    In order to remedy this issue additional requirements must be placed on the user, or at least
      offered to the user.
    The trade-off is that if there requirements are too cumbersome the user may opt not to use them
      or not to use the system that requires them. 
    This trade-off is the user-acceptability design principle.

    \begin{keybox}[Design Principle 2: User Acceptability]
      The usability of a system must not be so compromised in the name of security that
        the intended users accept the requirements imposed on them.
    \end{keybox}

  \subsection{Design Principle 3: System Complexity}
  \label{ssec:design_principle_3_system_complexity}

    Is it easier to analyze the security of a small program or a large one?
    The obvious answer is that smaller systems may be more easily analyzed than larger ones.
    But larger systems do provide the notable benefit of being more feature rich.
    Thus in a TCB it is necessary to balance these two trades.
    When possible fewer and simpler mechanisms should be used.
    However, this not always possible, so as systems get more complex more attention to 
      the security of the systems needs to be heeded.
    
    \begin{keybox}[Design Principle 3: System Complexity]
      A secure system should be as simple as possible while still achieving the
        necessary functionality.
    \end{keybox}

  \subsection{Design Principle 4: Least Privilege}
  \label{ssec:design_principle_4_least_privilege}

    When a user is running an application how do we decide what privileges that user and program 
      should have to interact with resources?
    Should they have the bare minimum required to complete their task?
    Probably.

    This is the principle of least privilege and it is the strongest principle from a security standpoint.
    However, in complex systems it can be tempting to bypass this system for a more convenient method of
      assigning privilege such as based on the user running the program, without regard for what the program
      actually needs.
    In fact this user-level privilege model is what Windows uses.
    Android on the other hand uses a stronger model much closer to least privilege as it grants permissions
      (i.e. privileges) based on what the application does in conjunction with what the user is willing to 
      let it do.
    However, even this is not perfect least privilege since the program could request privileges it doesn't
      actually need if the app is a bad actor.

    Ultimately, least privilege results in two key principles: separation of privileges and fail-safe default.
    Separation of privileges implies a finer grained access control.
    This might entail splitting files into different privileges.
    It may also include dividing different I/O devices in to different privilege requirements.
    Fail-safe defaults deny access when access is not explicitly granted.
    This means that some programs which are not granted a permission, but need it, could fail to execute properly.
    However, this is preferable to allowing malicious programs access to too much.

    \begin{keybox}[Design Principle 4: Least Privilege]
      A user and a program pair should be granted the least amount of privilege (i.e. access) to a system
        required in order to perform their approved task.
    \end{keybox}

  \subsection{Design Principle 5: Defense in Depth}
  \label{ssec:design_principle_5_defense_in_depth}

    Can any defensive layer be perfect?
    No (and anyone who tells you differently is trying to sell you something).
    Instead we must assume each layer has a set of weaknesses which an attacker may exploit.
    Each of these layers may share some weaknesses, but they may also cover others.
    Thus if we combine two layers we can cover some of the weaknesses of the system and force a malicious
      force to use various types of attack which significantly increases the attack complexity.
    The more layers which are stacked on each other the more this complexity increases and the security increases.
    Through this multi-layer construction of security we gain additional defense through the depth (number of layers)
      of security.

    \begin{keybox}[Design Principle 5: Defense In Depth]
      A secure program should use multiple methods of defense in order to cover the vulnerabilities of one layer
        with the strength of another layer whenever possible.
    \end{keybox}